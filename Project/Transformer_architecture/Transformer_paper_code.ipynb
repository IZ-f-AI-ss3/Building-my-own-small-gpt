{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import numpy as np\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.masked import masked_tensor\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module) :\n",
    "    def __init__(self, d_model, nhead = 1, dim_feedforward=2048, dropout=0.1, activation = F.relu) :\n",
    "        super(TransformerEncoderLayer,self).__init__()\n",
    "\n",
    "        self.Q_v = nn.Linear(in_features = d_model ,out_features = d_model ) # (token, d_model) -> (token, d_model)\n",
    "        self.K_v = nn.Linear(in_features = d_model ,out_features = d_model ) # (token, d_model) -> (token, d_model)\n",
    "        self.V_v = nn.Linear(in_features = d_model ,out_features = d_model ) # (token, d_model) -> (token, d_model)\n",
    "\n",
    "\n",
    "    def forward( self , x ) : # (batch_size, tokens, embedding_size)\n",
    "        key = self.K_v(x) # (batch_size, tokens, d_model)\n",
    "        query = self.Q_v(x) # (batch_size, tokens, d_model)\n",
    "        value = self.V_v(x) # (batch_size, tokens, d_model)\n",
    "        \n",
    "        scores = torch.sum(query.unsqueeze(1)*key.unsqueeze(2) , dim = 3) # (batch_size, tokens, tokens)\n",
    "        weights = F.softmax(scores, dim = -1) # (batch_size, tokens, tokens)\n",
    "\n",
    "        result = torch.matmul(weights , value )               # (batch_size, tokens, tokens) @ (batch_size, tokens, d_model)  \n",
    "\n",
    "        return result    \n",
    "\n",
    "\n",
    "    \n",
    "class MultiHeadAttention(nn.Module) :\n",
    "    def __init__(self, d_model, nhead = 4, dropout=0.1) :\n",
    "        super( MultiHeadAttention,self).__init__()\n",
    "        assert  d_model%nhead == 0\n",
    "        self.nhead =  nhead\n",
    "        self.d_model = d_model\n",
    "        self.Q_v = nn.Linear(in_features = d_model ,out_features = d_model)  # (token, d_model) -> (token, d_model)\n",
    "        self.K_v = nn.Linear(in_features = d_model ,out_features = d_model)  # (token, d_model) -> (token, d_model)\n",
    "        self.V_v = nn.Linear(in_features = d_model ,out_features = d_model ) # (token, d_model) -> (token, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward( self , x , mask_x ) : # (batch_size, tokens, d_model)\n",
    "        batch_size, seq_length =  x.size(0), x.size(1)\n",
    "        key   = self.K_v(x).reshape(batch_size, seq_length, self.nhead , self.d_model // self.nhead).transpose(1,2)   # (batch_size, n_head, tokens , d_model//n_head)\n",
    "        query = self.Q_v(x).reshape(batch_size, seq_length, self.nhead , self.d_model // self.nhead).transpose(1,2) # (batch_size, n_head, tokens , d_model//n_head)\n",
    "        value = self.V_v(x).reshape(batch_size, seq_length, self.nhead , self.d_model // self.nhead).transpose(1,2)    # (batch_size, n_head ,tokens , d_model//n_head)\n",
    "        \n",
    "        scores = torch.matmul( query , key.transpose(-2, -1))/ math.sqrt(self.d_model//self.nhead) # (batch_size, nhead, tokens, tokens)\n",
    "\n",
    "\n",
    "        Mask_x = (mask_x.unsqueeze(1) * mask_x.unsqueeze(2)).bool() # (batch, max_len, max_len)\n",
    "        mask_combined = ~(Mask_x.unsqueeze(1))\n",
    "        scores = scores.masked_fill(mask_combined , -float('inf'),)  # (batch_size, nhead, tokens, tokens)\n",
    "\n",
    "\n",
    "        weights = self.dropout(F.softmax(scores, dim = -1) )# (batch_size, nhead, tokens, tokens)\n",
    "        result = torch.matmul(weights , value ) # (batch_size, nhead, tokens, d_model//n_head)\n",
    "        result = torch.transpose(result, 1, 2)   # (batch_size, tokens, n_head, d_model//n_head)\n",
    "        result = result.reshape( batch_size, seq_length, self.d_model )\n",
    "        return result    \n",
    "    \n",
    "\n",
    "\n",
    "class MaskedMultiHeadAttention(nn.Module) :\n",
    "    def __init__(self, d_model, nhead = 4, dropout=0.1) :\n",
    "        super(MaskedMultiHeadAttention,self).__init__()\n",
    "        assert  d_model%nhead == 0\n",
    "        self.nhead =  nhead\n",
    "        self.d_model = d_model\n",
    "        self.Q_v = nn.Linear(in_features = d_model ,out_features = d_model)  # (token, d_model) -> (token, d_model)\n",
    "        self.K_v = nn.Linear(in_features = d_model ,out_features = d_model)  # (token, d_model) -> (token, d_model)\n",
    "        self.V_v = nn.Linear(in_features = d_model ,out_features = d_model ) # (token, d_model) -> (token, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.max_len = 1000\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones( 1 , 1 , self.max_len ,self.max_len) , diagonal= 1).bool() ) # The maximum lenght is 1000 here ! \n",
    "\n",
    "    def forward( self , x , mask_x) : # (batch_size, tokens, d_model)  & (batch_size, max_len)\n",
    "        batch_size, seq_length =  x.size(0), x.size(1)\n",
    "        key   = self.K_v(x).reshape(batch_size, seq_length, self.nhead , self.d_model // self.nhead).transpose(1,2)   # (batch_size, n_head, tokens , d_model//n_head)\n",
    "        query = self.Q_v(x).reshape(batch_size, seq_length, self.nhead , self.d_model // self.nhead).transpose(1,2)   # (batch_size, n_head, tokens , d_model//n_head)\n",
    "        value = self.V_v(x).reshape(batch_size, seq_length, self.nhead , self.d_model // self.nhead).transpose(1,2)   # (batch_size, n_head ,tokens , d_model//n_head)\n",
    "            \n",
    "        scores = torch.matmul( query , key.transpose(-2, -1))/ math.sqrt(self.d_model//self.nhead) # (batch_size, nhead, tokens, tokens)\n",
    "    #   if mask_x is not None:\n",
    "\n",
    "        Mask = self.mask[:,:, :seq_length, :seq_length] # ( 1, 1 , max_len, max_len)\n",
    "\n",
    "\n",
    "        Mask_x = mask_x.unsqueeze(1) * mask_x.unsqueeze(2) # (batch, max_len, max_len)\n",
    "        mask_combined = Mask | ~(Mask_x.unsqueeze(1)).bool()\n",
    "        scores = scores.masked_fill(mask_combined , -float('inf'),)  # (batch_size, nhead, tokens, tokens)\n",
    "\n",
    "\n",
    "        weights = F.softmax(scores, dim = -1) # (batch_size, nhead, tokens, tokens)\n",
    "        weights =  self.dropout(weights)\n",
    "\n",
    "        result = torch.matmul(weights , value ) # (batch_size, nhead, tokens, d_model//n_head)\n",
    "        result = torch.transpose(result, 1, 2)   # (batch_size, tokens, n_head, d_model//n_head)\n",
    "        result = result.reshape( batch_size, seq_length, self.d_model )  # (batch_size, tokens, d_model)\n",
    "        return result \n",
    "    \n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module) :\n",
    "    def __init__(self, d_model, nhead = 4, dim_feedforward= 128, dropout=0.1, activation = nn.ReLU) :\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.multi_head_att = MultiHeadAttention(d_model, nhead = nhead, dropout=dropout)\n",
    "        self.feed_fw = nn.Sequential(\n",
    "                                nn.Linear(in_features = d_model ,out_features = dim_feedforward),\n",
    "                                activation() ,\n",
    "                                nn.Linear(in_features=dim_feedforward , out_features=d_model)\n",
    "                                    )\n",
    "                                     \n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self , x , mask_x) :  # (batch_size, token, dim)\n",
    "        x =  self.layer_norm1(x +  self.multi_head_att(x , mask_x))  # (batch_size, token, dim)\n",
    "        x =  self.layer_norm2(x + self.feed_fw(x))          # (batch_size, token, dim)\n",
    "        return x\n",
    "\n",
    "class CustomMultiHeadAttention(nn.Module) :\n",
    "    def __init__(self, d_model, nhead = 4, dropout=0.1) :\n",
    "        super( CustomMultiHeadAttention,self).__init__()\n",
    "        assert  d_model%nhead == 0\n",
    "        self.nhead =  nhead\n",
    "        self.d_model = d_model\n",
    "        self.Q_v = nn.Linear(in_features = d_model ,out_features = d_model)  # (token, d_model) -> (token, d_model)\n",
    "        self.K_v = nn.Linear(in_features = d_model ,out_features = d_model)  # (token, d_model) -> (token, d_model)\n",
    "        self.V_v = nn.Linear(in_features = d_model ,out_features = d_model ) # (token, d_model) -> (token, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward( self , x , y  ) : # (batch_size, tokens, d_model) x : output of encoder, y output of masked_multi_head\n",
    "        assert x.shape == y.shape\n",
    "        batch_size, seq_length =  x.size(0), x.size(1)\n",
    "        key   = self.K_v(x).reshape(batch_size, seq_length, self.nhead , self.d_model // self.nhead).transpose(1,2)   # (batch_size, n_head, tokens , d_model//n_head)\n",
    "        query = self.Q_v(y).reshape(batch_size, seq_length, self.nhead , self.d_model // self.nhead).transpose(1,2) # (batch_size, n_head, tokens , d_model//n_head)\n",
    "        value = self.V_v(x).reshape(batch_size, seq_length, self.nhead , self.d_model // self.nhead).transpose(1,2)    # (batch_size, n_head ,tokens , d_model//n_head)\n",
    "        \n",
    "        scores = torch.matmul( query , key.transpose(-2, -1))/ math.sqrt(self.d_model//self.nhead) # (batch_size, nhead, tokens, tokens)\n",
    "        weights = self.dropout(F.softmax(scores, dim = -1) )# (batch_size, nhead, tokens, tokens)\n",
    "        result = torch.matmul(weights , value ) # (batch_size, nhead, tokens, d_model//n_head)\n",
    "        result = torch.transpose(result, 1, 2)   # (batch_size, tokens, n_head, d_model//n_head)\n",
    "        result = result.reshape( batch_size, seq_length, self.d_model )\n",
    "        return result        \n",
    "    \n",
    "\n",
    "\n",
    "class TransformerEncoderDecoder(nn.Module) :\n",
    "    def __init__(self, d_model, nhead = 4, dim_feedforward= 128, dropout=0.1, activation = nn.ReLU) :\n",
    "        super(TransformerEncoderDecoder, self).__init__()\n",
    "        self.transformer_encoder = TransformerEncoder(d_model=d_model , nhead=nhead, dim_feedforward= dim_feedforward, dropout=dropout)\n",
    "        self.masked_multi_head_att =  MaskedMultiHeadAttention(d_model, nhead = nhead, dropout=dropout)\n",
    "        self.multi_head_att = CustomMultiHeadAttention(d_model, nhead = nhead, dropout=dropout)\n",
    "\n",
    "        self.feed_fw = nn.Sequential(\n",
    "                                nn.Linear(in_features = d_model ,out_features = dim_feedforward),\n",
    "                                activation() ,\n",
    "                                nn.Linear(in_features=dim_feedforward , out_features=d_model)\n",
    "                                    )\n",
    "                                     \n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        \n",
    "    def forward(self , x , y , x_mask, y_mask) :  # (batch_size, token, dim) & (batch_size, max_len)\n",
    "        assert x.shape == y.shape\n",
    "        # ENCODER\n",
    "        x = self.transformer_encoder(x , x_mask) # (batch_size, token, dim)\n",
    "        # DECODER \n",
    "        y =  self.layer_norm1(y +  self.masked_multi_head_att(y , y_mask))  # (batch_size, token, dim)\n",
    "        z = self.multi_head_att(x,y) # (batch_size, token, dim)\n",
    "        z = self.layer_norm2( y + z ) # (batch_size, token, dim)\n",
    "        z =  self.layer_norm3(z + self.feed_fw(z)) # (batch_size, token, dim)\n",
    "\n",
    "        return z   # (batch_size, token, dim)\n",
    "    \n",
    "\n",
    "class Transformer(nn.Module) :\n",
    "    def __init__(self, embedding_dim = 512, max_len = 1000 , vocab_size = 30522 , nhead = 8, dim_feedforward = 2048, dropout=0.1, activation = nn.ReLU) :\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = nn.Embedding( num_embeddings = vocab_size, embedding_dim = embedding_dim ) #optim_SGD\n",
    "        self.Transformer_encoder_decoder = TransformerEncoderDecoder(d_model=embedding_dim, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation)\n",
    "        self.linear_layer = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "        self.linear_layer.weight  = self.embedding.weight #  (embedding_dim --> vocab_size )\n",
    "\n",
    "        P = torch.zeros(max_len, embedding_dim) \n",
    "        position = torch.arange(0,  max_len, dtype=torch.float).unsqueeze(1)  # Shape: (seq_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * -(math.log(10000.0) / embedding_dim))\n",
    "\n",
    "        P[:, 0::2] = torch.sin(position * div_term)  # sinus to even indices\n",
    "        P[:, 1::2] = torch.cos(position * div_term)  # cosinus to odd indices\n",
    "\n",
    "\n",
    "        self.register_buffer(\"positional_encoding\",  P ) # The maximum lenght is max_len here !  \n",
    "\n",
    "    def forward(self, input_ids , target_ids , input_mask , target_mask ) :   # ids are in  (batch_size, seq_len)\n",
    "        seq_len = input_ids.size(1)\n",
    "\n",
    "        \n",
    "        x = self.embedding(input_ids) #  (batch_size, seq_len , dim)\n",
    "        x = x + self.positional_encoding[ :seq_len, :].unsqueeze(0).to(x.device)\n",
    "        y = self.embedding(target_ids) #  (batch_size, seq_len , dim)\n",
    "        y = y + self.positional_encoding[ :seq_len, :].unsqueeze(0).to(x.device)\n",
    "\n",
    "        z = self.Transformer_encoder_decoder(x, y , input_mask , target_mask) #  (batch_size, seq_len , dim)\n",
    "        z = self.linear_layer(z)    #  (batch_size, seq_len , vocab_size)\n",
    "\n",
    "        # z = F.softmax(z , dim = -1)   #  (batch_size, seq_len , vocab_size)\n",
    "\n",
    "        # I will remove it if I use cross entropy loss ...\n",
    "\n",
    "        return z  #  (batch_size, seq_len , vocab_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Workspace/.venv1/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Replace 'bert-base-uncased' with the model you are using\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example\n",
    "input_text = \"What is the weather like today?\"\n",
    "\n",
    "\n",
    "tokens = tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=32, return_tensors=\"pt\")\n",
    "\n",
    "# print(tokens['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quel temps fait-il aujourd'hui ?</td>\n",
       "      <td>The weather is sunny and warm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traduisez cette phrase en français.</td>\n",
       "      <td>Traduisez cette phrase en français.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comment allez-vous ?</td>\n",
       "      <td>I’m doing well, thank you!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Écrivez une courte histoire sur un robot.</td>\n",
       "      <td>Once upon a time, there was a robot who wanted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Résumez l'article suivant.</td>\n",
       "      <td>This article discusses the importance of AI in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  input_text  \\\n",
       "0           Quel temps fait-il aujourd'hui ?   \n",
       "1        Traduisez cette phrase en français.   \n",
       "2                       Comment allez-vous ?   \n",
       "3  Écrivez une courte histoire sur un robot.   \n",
       "4                 Résumez l'article suivant.   \n",
       "\n",
       "                                         target_text  \n",
       "0                     The weather is sunny and warm.  \n",
       "1                Traduisez cette phrase en français.  \n",
       "2                         I’m doing well, thank you!  \n",
       "3  Once upon a time, there was a robot who wanted...  \n",
       "4  This article discusses the importance of AI in...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "train_dataset = {\n",
    "    \"input_text\": [\n",
    "        \"Quel temps fait-il aujourd'hui ?\" ,\n",
    "        \"Traduisez cette phrase en français.\",\n",
    "        \"Comment allez-vous ?\",\n",
    "        \"Écrivez une courte histoire sur un robot.\",\n",
    "        \"Résumez l'article suivant.\",\n",
    "    ],\n",
    "    \"target_text\": [\n",
    "        \"The weather is sunny and warm.\",\n",
    "        \"Traduisez cette phrase en français.\",\n",
    "        \"I’m doing well, thank you!\",\n",
    "        \"Once upon a time, there was a robot who wanted to be human.\",\n",
    "        \"This article discusses the importance of AI in modern technology.\",\n",
    "    ]\n",
    "}\n",
    "vocab_set = list(set( ''.join(train_dataset['input_text'])+ ''.join(train_dataset['target_text'])))\n",
    "vocab_size = len(vocab_set)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenize = lambda txt : tokenizer(txt, padding=\"max_length\", truncation=True, max_length=32, return_tensors=\"pt\")\n",
    "# vocab_size = tokenizer.vocab_size\n",
    "\n",
    "token_to_id = { v:i for  i , v in enumerate(vocab_set)}\n",
    "id_to_token = { i:v for  i , v in enumerate(vocab_set)}\n",
    "tokenize = lambda txt : torch.tensor( [ token_to_id[txt[i]] for i in range(len(txt))])\n",
    "# tokenizer_decode =  lambda txt : ''.join([ id_to_token[txt[i]] for i in range(len(txt))])\n",
    "\n",
    "training_corpus = pd.DataFrame(train_dataset) \n",
    "\n",
    "training_corpus.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset) :\n",
    "    def __init__(self , training_corpus , tokenize) :\n",
    "        self.training_corpus = training_corpus\n",
    "        # self.training_corpus['input_token'] = self.training_corpus['input_text'].apply(lambda txt : tokenizer(txt, padding=\"max_length\", truncation=True, max_length=32, return_tensors=\"pt\") )\n",
    "        # self.training_corpus['target_token'] = self.training_corpus['target_text'].apply(lambda txt : tokenizer(txt, padding=\"max_length\", truncation=True, max_length=32, return_tensors=\"pt\") )\n",
    "        self.training_corpus['input_token'] = self.training_corpus['input_text'].apply(lambda txt : tokenize(txt) )\n",
    "        self.training_corpus['target_token'] = self.training_corpus['target_text'].apply(lambda txt : tokenize(txt) )\n",
    "\n",
    "    def __getitem__(self, index) :\n",
    "        row =  self.training_corpus.iloc[index]\n",
    "        input = row['input_token']\n",
    "        target = row['target_token']\n",
    "   \n",
    "        # return input['input_ids'].squeeze(0), target['input_ids'].squeeze(0), input['attention_mask'].squeeze(0), target['attention_mask'].squeeze(0)\n",
    "        # return input['input_ids'].squeeze(0), target['input_ids'].squeeze(0), input['attention_mask'].squeeze(0), target['attention_mask'].squeeze(0)\n",
    "        return input, target\n",
    "    \n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.training_corpus)\n",
    "    \n",
    "def my_custom_collater(batch) :\n",
    "    inputs = [ e[0]  for e in batch ]\n",
    "    targets = [ e[1]  for e in batch ]\n",
    "    input_len = [len(inp) for inp in inputs]\n",
    "    target_len = [len(targ) for targ in targets]\n",
    "    max_len_t = max(input_len)\n",
    "    max_len_i = max(target_len)\n",
    "    max_len = max(max_len_t,max_len_i)\n",
    "\n",
    "    # Padding \n",
    "    pad_value = 0\n",
    "    padded_inputs = torch.stack([F.pad(seq, (0,max_len - seq.size(0)) , mode='constant', value=pad_value)  for seq in  inputs] )\n",
    "    padded_targets = torch.stack([F.pad(seq, (0 , max_len - seq.size(0)), mode='constant', value=pad_value)   for seq in  targets] )\n",
    "    input_mask = (padded_inputs != pad_value).long()  # (batch_size, max_len)\n",
    "    output_mask = (padded_inputs != pad_value).long() # (batch_size, max_len)\n",
    "\n",
    "    return padded_inputs , padded_targets , input_mask , output_mask \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # return padded_inputs, padded_targets, input_mask, output_mask\n",
    "\n",
    "train_dataset = CustomDataset(training_corpus , tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: nan\n",
      "Epoch 2/10, Loss: nan\n",
      "Epoch 3/10, Loss: nan\n",
      "Epoch 4/10, Loss: nan\n",
      "Epoch 5/10, Loss: nan\n",
      "Epoch 6/10, Loss: nan\n",
      "Epoch 7/10, Loss: nan\n",
      "Epoch 8/10, Loss: nan\n",
      "Epoch 9/10, Loss: nan\n",
      "Epoch 10/10, Loss: nan\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='none')       \n",
    "model = Transformer(\n",
    "    embedding_dim=64, vocab_size=vocab_size, nhead=8,\n",
    "    dim_feedforward=128, dropout=0.1, activation=nn.GELU\n",
    ")\n",
    "# model = Transformer(embedding_dim = 512, vocab_size = vocab_size , nhead = 8, dim_feedforward = 2048, dropout=0.1, activation = nn.GELU) \n",
    "\n",
    "\n",
    "data_loader = DataLoader(train_dataset, batch_size=2, shuffle=False, collate_fn=my_custom_collater)\n",
    "optimizer = optim.SGD(model.parameters() , lr = 1e-4 )\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        input, target, input_mask, target_mask = batch  # Assume custom collate returns these\n",
    "\n",
    "        # Forward pass\n",
    "        result = model(input, target, input_mask, target_mask)  # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        batch_size, seq_len, vocab_size = input.size(0), input.size(1), result.size(2)\n",
    "        result = result.reshape(batch_size * seq_len, vocab_size)  # (batch_size * seq_len, vocab_size)\n",
    "        target = target.view(-1)  # (batch_size * seq_len)\n",
    "        input_mask = input_mask.view(-1).float()  # Flatten and cast mask to float\n",
    "\n",
    "        # Compute loss\n",
    "        optimizer.zero_grad()\n",
    "        loss = (criterion(result, target) * input_mask).sum() / input_mask.sum()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight gradient: nan\n",
      "Transformer_encoder_decoder.transformer_encoder.multi_head_att.Q_v.weight gradient: nan\n",
      "Transformer_encoder_decoder.transformer_encoder.multi_head_att.Q_v.bias gradient: nan\n",
      "Transformer_encoder_decoder.transformer_encoder.multi_head_att.K_v.weight gradient: nan\n",
      "Transformer_encoder_decoder.transformer_encoder.multi_head_att.K_v.bias gradient: nan\n",
      "Transformer_encoder_decoder.transformer_encoder.multi_head_att.V_v.weight gradient: nan\n",
      "Transformer_encoder_decoder.transformer_encoder.multi_head_att.V_v.bias gradient: nan\n",
      "Transformer_encoder_decoder.transformer_encoder.feed_fw.0.weight gradient: nan\n",
      "Transformer_encoder_decoder.transformer_encoder.feed_fw.0.bias gradient: nan\n",
      "Transformer_encoder_decoder.transformer_encoder.feed_fw.2.weight gradient: nan\n",
      "Transformer_encoder_decoder.transformer_encoder.feed_fw.2.bias gradient: nan\n",
      "Transformer_encoder_decoder.transformer_encoder.layer_norm1.weight gradient: nan\n",
      "Transformer_encoder_decoder.transformer_encoder.layer_norm1.bias gradient: nan\n",
      "Transformer_encoder_decoder.transformer_encoder.layer_norm2.weight gradient: nan\n",
      "Transformer_encoder_decoder.transformer_encoder.layer_norm2.bias gradient: nan\n",
      "Transformer_encoder_decoder.masked_multi_head_att.Q_v.weight gradient: nan\n",
      "Transformer_encoder_decoder.masked_multi_head_att.Q_v.bias gradient: nan\n",
      "Transformer_encoder_decoder.masked_multi_head_att.K_v.weight gradient: nan\n",
      "Transformer_encoder_decoder.masked_multi_head_att.K_v.bias gradient: nan\n",
      "Transformer_encoder_decoder.masked_multi_head_att.V_v.weight gradient: nan\n",
      "Transformer_encoder_decoder.masked_multi_head_att.V_v.bias gradient: nan\n",
      "Transformer_encoder_decoder.multi_head_att.Q_v.weight gradient: nan\n",
      "Transformer_encoder_decoder.multi_head_att.Q_v.bias gradient: nan\n",
      "Transformer_encoder_decoder.multi_head_att.K_v.weight gradient: nan\n",
      "Transformer_encoder_decoder.multi_head_att.K_v.bias gradient: nan\n",
      "Transformer_encoder_decoder.multi_head_att.V_v.weight gradient: nan\n",
      "Transformer_encoder_decoder.multi_head_att.V_v.bias gradient: nan\n",
      "Transformer_encoder_decoder.feed_fw.0.weight gradient: nan\n",
      "Transformer_encoder_decoder.feed_fw.0.bias gradient: nan\n",
      "Transformer_encoder_decoder.feed_fw.2.weight gradient: nan\n",
      "Transformer_encoder_decoder.feed_fw.2.bias gradient: nan\n",
      "Transformer_encoder_decoder.layer_norm1.weight gradient: nan\n",
      "Transformer_encoder_decoder.layer_norm1.bias gradient: nan\n",
      "Transformer_encoder_decoder.layer_norm2.weight gradient: nan\n",
      "Transformer_encoder_decoder.layer_norm2.bias gradient: nan\n",
      "Transformer_encoder_decoder.layer_norm3.weight gradient: nan\n",
      "Transformer_encoder_decoder.layer_norm3.bias gradient: nan\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name} gradient: {param.grad.abs().max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
